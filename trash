
// int check_for_operators(t_token **tokens, int index_start, )
// {
// 	int i;
// 	t_token *tmp;
// 	int len;

// 	tmp = *tokens;
// 	len = len_ll_list(tokens);
// 	if (index_start >= len)
// 		return (-1);
// 	while (tmp)
// 	{
// 		if (tmp->index == index_start)
// 			break;
// 		tmp = tmp->next;
// 	}
// 	while (tmp)
// 	{
// 		if (tmp->token == TOK_OPERATOR)
// 			return (tmp->index);
// 		tmp = tmp->next;
// 	}
// 	return (-2);
// }



// int	is_there_oth_op(t_token *tokens)
// {
// 	t_token *tmp;

// 	tmp = tokens;
// 	while (tmp)
// 	{
// 		if (check_divider_type(tmp->token) > 0)
// 			return (tmp->index);
// 		tmp = tmp->next;
// 	}
// 	return (-1);
// }



// void	browse_tokens(t_token **tokens)
// {
// 	t_token	*tmp;
// 	int		jump;
// 	int		par;
// 	int		and_for_none;

// 	par = 0;
// 	tmp = *tokens;
// 	and_for_none = 0;
// 	while (tmp)
// 	{
// 		if (tmp->value == TOK_EXPANDER_OP)
// 		{
// 			jump = cl_par_ind(tokens, tmp->index);
// 			if ((jump + 1) >= len_ll_list(tokens))
// 			{
// 				move_tok_2_ind(&tmp, jump);
// 				tmp = tmp->next;
// 				//create wahed kayn entre parentheses
// 			}
// 			else
// 			{
// 				//create the big token with arenthesis (it'll be a clear token) avec valeur de start et de end (that would be the last token of your tokens)
// 			}
// 		}
// 		else if (check_divider_type(tmp->value) > 0)
// 		{
// 			if (is_there_oth_op(tokens) > -1)
// 			{
// 				//create chnou binathoum
// 				//n9ez lmn b3d ma tsala segment
// 			}
// 			else
// 			{
// 				//create a big token ghadi htal lkher
// 			}
// 		}
// 		else
// 			//ma3reftch
// 	}

// }

// void divide_b_toks(t_token **tokens)
// {
// 	int index_start;

// 	index_start = 0;
// 	while (1)
// 	{
// 		if (check_for_operators(tokens) > 0)
// 	}
// }

// void add_b_tok_child_last(t_big_token **b_tok_list, t_big_tok_type type, int start, int length)
// {
// 	t_big_token *tmp;
// 	t_big_token *bef_last;
// 	int rank_in_list;

// 	if (!*b_tok_list)
// 	{
// 		*b_tok_list = ft_create_btoken(type, start, length);
// 		rank_in_list = 0;
// 	}
// 	else
// 	{
// 		tmp = *b_tok_list;
// 		while (tmp->sibling)
// 			tmp = tmp->sibling;
// 		bef_last = tmp;
// 		bef_last->child = ft_create_btoken(type, start, length);
// 		rank_in_list = 1;
// 	}
// init_tok_struct(tok_list, rank_in_list);
// }

void print_b_tokens(t_big_token *b_token, t_token *tokens_s, int start, int length)
// {
// 	int i;
// 	int j;
// 	t_big_token	*tmp_b;
// 	t_token	*tokens;

// 	// printf ("this is the glabal length of toks: %d\n", len_ll_list(tokens));
// 	tmp_b = b_token;
// 	tokens = tokens_s;
// 	while (tmp_b)
// 	{
// 		j = 0;
// 		printf("Here is a big token :\n");
// 		printf("this is the start of the b_token : %d\n", tmp_b->ind_tok_start);
// 		printf("this is the length of the b_token : %d\n", tmp_b->length);
// 		printf("this is the par of the b_token : %d\n", tmp_b->par);
// 		i = tmp_b->ind_tok_start;
// 		printf("\n\ni : %d\n\n", i);
// 		printf("\n\ntmp->b->len: %d\n\n", tmp_b->length);
// 		while (j < tmp_b->length)
// 		{
// //			printf("\n\nj: %d\n\n", j);
// 			move_tok_2_ind(&tokens, i);
// //			printf("\n\nindex: %d\n\n", tokens->index);
// 			printf("%s", tokens->value);
// 			i++;
// 			j++;
// 		}
// 		printf("\n\n");
// 		tmp_b = tmp_b->sibling;
// 	}
// }
